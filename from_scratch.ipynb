{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature      # Which feature to split on\n",
    "        self.threshold = threshold  # Value of split\n",
    "        self.left = left          # Left child\n",
    "        self.right = right        # Right child\n",
    "        self.value = value        # For leaf nodes, store the predicted class\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, criterion='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.criterion = criterion  # 'gini' or 'entropy'\n",
    "        self.root = None\n",
    "    \n",
    "    def _gini(self, y):\n",
    "        \"\"\"Calculate Gini impurity for a node\n",
    "        \n",
    "        Args:\n",
    "            y: Array of class labels at the node\n",
    "        Returns:\n",
    "            Gini impurity value\n",
    "        \"\"\"\n",
    "        # Get count of each class\n",
    "        counts = np.bincount(y)\n",
    "        # Convert to probabilities\n",
    "        probabilities = counts / len(y)\n",
    "        # Calculate Gini: 1 - Σ(pi²)\n",
    "        return 1 - np.sum(probabilities ** 2)\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        \"\"\"Calculate entropy for a node\n",
    "        \n",
    "        Args:\n",
    "            y: Array of class labels at the node\n",
    "        Returns:\n",
    "            Entropy value\n",
    "        \"\"\"\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        # Remove zero probabilities to avoid log(0)\n",
    "        probabilities = probabilities[probabilities > 0]\n",
    "        # Calculate entropy: -Σ(pi * log2(pi))\n",
    "        return -np.sum(probabilities * np.log2(probabilities))\n",
    "    \n",
    "    def _criterion_function(self, y):\n",
    "        \"\"\"Wrapper to use either gini or entropy based on initialization\"\"\"\n",
    "        if self.criterion == 'gini':\n",
    "            return self._gini(y)\n",
    "        else:\n",
    "            return self._entropy(y)\n",
    "    \n",
    "    def _information_gain(self, parent, left_child, right_child):\n",
    "        \"\"\"Calculate information gain for a split\n",
    "        \n",
    "        Args:\n",
    "            parent: Array of class labels before split\n",
    "            left_child: Array of class labels in left node after split\n",
    "            right_child: Array of class labels in right node after split\n",
    "        Returns:\n",
    "            Information gain value\n",
    "        \"\"\"\n",
    "        # Calculate weights for weighted average\n",
    "        w_left = len(left_child) / len(parent)\n",
    "        w_right = len(right_child) / len(parent)\n",
    "        \n",
    "        # Calculate gain = parent impurity - weighted avg of children impurity\n",
    "        gain = self._criterion_function(parent) - (\n",
    "            w_left * self._criterion_function(left_child) +\n",
    "            w_right * self._criterion_function(right_child)\n",
    "        )\n",
    "        \n",
    "        return gain\n",
    "\n",
    "    def _should_stop(self, depth, X, y):\n",
    "        \"\"\"Check if tree building should stop at this node\n",
    "        \n",
    "        Args:\n",
    "            depth: Current depth of the tree\n",
    "            X: Feature matrix for samples at this node\n",
    "            y: Labels for samples at this node\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if we should stop, False otherwise\n",
    "        \"\"\"\n",
    "        # 1. Max depth reached\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return True\n",
    "        \n",
    "        # 2. Not enough samples to split\n",
    "        if len(y) < self.min_samples_split:\n",
    "            return True\n",
    "        \n",
    "        # 3. Node is pure (all samples belong to same class)\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return True\n",
    "        \n",
    "        # 4. No valid splits possible (all features have same value)\n",
    "        if np.all(np.all(X == X[0, :], axis=0)):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"Find the best split for a node.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix for samples at this node\n",
    "            y: Labels for samples at this node\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "            - 'feature': Best feature to split on\n",
    "            - 'threshold': Best threshold value\n",
    "            - 'gain': Information gain from this split\n",
    "            - 'left_idxs': Indices of samples going to left child\n",
    "            - 'right_idxs': Indices of samples going to right child\n",
    "        \"\"\"\n",
    "        best_split = {\n",
    "            'feature': None,\n",
    "            'threshold': None,\n",
    "            'gain': -float('inf'),\n",
    "            'left_idxs': None,\n",
    "            'right_idxs': None\n",
    "        }\n",
    "        \n",
    "        # Need at least min_samples_split samples to consider splitting\n",
    "        if len(y) < self.min_samples_split:\n",
    "            return None\n",
    "        \n",
    "        # Try each feature\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        for feature in range(n_features):\n",
    "            # Get unique values in this feature\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            \n",
    "            # Try each threshold\n",
    "            for threshold in thresholds:\n",
    "                # Split the data\n",
    "                left_idxs = X[:, feature] <= threshold\n",
    "                right_idxs = ~left_idxs\n",
    "                \n",
    "                # Skip if split doesn't meet minimum samples requirement\n",
    "                if (np.sum(left_idxs) < self.min_samples_leaf or \n",
    "                    np.sum(right_idxs) < self.min_samples_leaf):\n",
    "                    continue\n",
    "                \n",
    "                # Calculate information gain\n",
    "                gain = self._information_gain(\n",
    "                    y,\n",
    "                    y[left_idxs],\n",
    "                    y[right_idxs]\n",
    "                )\n",
    "                \n",
    "                # Update best split if this split is better\n",
    "                if gain > best_split['gain']:\n",
    "                    best_split['feature'] = feature\n",
    "                    best_split['threshold'] = threshold\n",
    "                    best_split['gain'] = gain\n",
    "                    best_split['left_idxs'] = left_idxs\n",
    "                    best_split['right_idxs'] = right_idxs\n",
    "        \n",
    "        return best_split if best_split['feature'] is not None else None\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0, debug=False):\n",
    "        \"\"\"Recursively build the decision tree\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix for samples at this node\n",
    "            y: Labels for samples at this node\n",
    "            depth: Current depth in tree (default=0 for root)\n",
    "        \n",
    "        Returns:\n",
    "            Node: Root node of the tree/subtree\n",
    "        \"\"\"\n",
    "        if debug:\n",
    "            print(f\"\\nAt depth {depth}:\")\n",
    "            print(f\"Sample count: {len(y)}\")\n",
    "            print(f\"Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "        # Check stopping conditions\n",
    "        if self._should_stop(depth, X, y):\n",
    "            # Create leaf node with majority class\n",
    "            if debug:\n",
    "                print(\"Stopping condition met! Creating leaf node.\")\n",
    "            majority_class = np.bincount(y).argmax()\n",
    "            return Node(value=majority_class)\n",
    "        \n",
    "        # Find the best split\n",
    "        best_split = self._best_split(X, y)\n",
    "        \n",
    "        # If no valid split found, return leaf node\n",
    "        if best_split is None:\n",
    "            if debug:\n",
    "                print(\"No valid split found! Creating leaf node.\")\n",
    "            majority_class = np.bincount(y).argmax()\n",
    "            return Node(value=majority_class)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Best split found:\")\n",
    "            print(f\"- Feature: {best_split['feature']}\")\n",
    "            print(f\"- Threshold: {best_split['threshold']}\")\n",
    "            print(f\"- Information gain: {best_split['gain']:.4f}\")\n",
    "\n",
    "        # Get indices for left and right children\n",
    "        left_idxs = best_split['left_idxs']\n",
    "        right_idxs = best_split['right_idxs']\n",
    "        \n",
    "        # Recursively build left and right subtrees\n",
    "        left_subtree = self._build_tree(\n",
    "            X[left_idxs],\n",
    "            y[left_idxs],\n",
    "            depth + 1,\n",
    "            debug\n",
    "        )\n",
    "        \n",
    "        right_subtree = self._build_tree(\n",
    "            X[right_idxs],\n",
    "            y[right_idxs],\n",
    "            depth + 1,\n",
    "            debug\n",
    "        )\n",
    "        \n",
    "        # Create and return decision node\n",
    "        return Node(\n",
    "            feature=best_split['feature'],\n",
    "            threshold=best_split['threshold'],\n",
    "            left=left_subtree,\n",
    "            right=right_subtree\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y, debug=False):\n",
    "        \"\"\"Fit the decision tree to the training data\n",
    "        \n",
    "        Args:\n",
    "            X: Training feature matrix\n",
    "            y: Training labels\n",
    "        \"\"\"\n",
    "        # Convert X to numpy array if it's a DataFrame\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        # Convert y to numpy array if it's a pandas Series\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.root = self._build_tree(X, y, depth=0, debug=debug)\n",
    "\n",
    "    def _traverse_tree(self, x, node, depth=0, debug=False):\n",
    "        \"\"\"Traverse the tree for a single sample to find its leaf node prediction\n",
    "        \n",
    "        Args:\n",
    "            x: Single sample features (array of length n_features)\n",
    "            node: Current node in traversal (starts from root)\n",
    "        \n",
    "        Returns:\n",
    "            Predicted class value\n",
    "        \"\"\"\n",
    "        if debug:\n",
    "            indent = \"  \" * depth  # Indent based on depth\n",
    "            if node.value is not None:\n",
    "                print(f\"{indent}Leaf Node! Predicting class: {node.value}\")\n",
    "            else:\n",
    "                feature_val = x[node.feature]\n",
    "                print(f\"{indent}At depth {depth}:\")\n",
    "                print(f\"{indent}Checking feature {node.feature} (value={feature_val}) against threshold {node.threshold}\")\n",
    "                print(f\"{indent}Going {'left' if feature_val <= node.threshold else 'right'}\")\n",
    "    \n",
    "        # If we've reached a leaf node, return its value\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        \n",
    "        # Decide whether to go left or right based on the split\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left, depth + 1, debug)\n",
    "        else:\n",
    "            return self._traverse_tree(x, node.right, depth + 1, debug)\n",
    "\n",
    "    def predict(self, X, debug=False):\n",
    "        \"\"\"Predict class labels for multiple samples\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix of samples to predict\n",
    "        \n",
    "        Returns:\n",
    "            Array of predicted class labels\n",
    "        \"\"\"\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        # Make predictions for each sample\n",
    "        predictions = []\n",
    "        for i, x in enumerate(X):\n",
    "            if debug:\n",
    "                print(f\"\\nPredicting sample {i}:\")\n",
    "            pred = self._traverse_tree(x, self.root, debug=debug)\n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "poker = pd.read_csv(\n",
    "    'poker-hand-training-true.data', \n",
    "    header=None, \n",
    "    names=['S1', 'C1', 'S2', 'C2', 'S3', 'C3', 'S4', 'C4', 'S5', 'C5', 'CLASS']\n",
    ")\n",
    "\n",
    "poker_test = pd.read_csv(\n",
    "    'poker-hand-testing.data', \n",
    "    header=None, \n",
    "    names=['S1', 'C1', 'S2', 'C2', 'S3', 'C3', 'S4', 'C4', 'S5', 'C5', 'CLASS']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feats_with_card_counts(df):\n",
    "    ''' Create features for the poker hand dataset and with highest and 2nd highest card counts '''\n",
    "    df_copy = df.copy()\n",
    "    # count cards of same rank\n",
    "    for card in range (1, 14):\n",
    "        df_copy[f'card {card}'] = df_copy[[f'C{i}' for i in range(1, 6)]].eq(card).sum(axis=1)\n",
    "    # count cards of same suit\n",
    "    for suit in range (1, 5):\n",
    "        df_copy[f'suit {suit}'] = df_copy[[f'S{i}' for i in range(1, 6)]].eq(suit).sum(axis=1)\n",
    "    # check sequential\n",
    "    df_copy['sorted_rank'] = df_copy.apply(lambda row: sorted([row[f'C{i}'] for i in range(1, 6)]), axis=1)\n",
    "    df_copy['is_sequental'] = df_copy['sorted_rank'].apply(lambda x: all(x[i+1] - x[i] == 1 for i in range(len(x)-1)))\n",
    "    df_copy['is_sequental'] = df_copy.apply(\n",
    "        lambda row: True if row['sorted_rank'] == [1, 10, 11, 12, 13] else row['is_sequental'], axis=1\n",
    "    )\n",
    "    # check flush\n",
    "    df_copy['is_flush'] = df_copy.apply(lambda row: any(row[f'suit {i}'] == 5 for i in range(1, 5)), axis=1)\n",
    "    # Identify highest card count\n",
    "    df_copy['max_card_count'] = df_copy[[f'card {i}' for i in range(1, 14)]].max(axis=1)\n",
    "    # Identify second highest card count\n",
    "    df_copy['second_max_card_count'] = df_copy.apply(\n",
    "        lambda x: sorted([x[f'card {i}'] for i in range(1, 14)])[-2], \n",
    "        axis=1\n",
    "    )\n",
    "    df_copy.drop(['S1', 'C1', 'S2', 'C2', 'S3', 'C3', 'S4', 'C4', 'S5', 'C5', 'sorted_rank'], axis=1, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "poker_after = create_feats_with_card_counts(poker)\n",
    "poker_test_after = create_feats_with_card_counts(poker_test)\n",
    "\n",
    "x_train = poker_after.drop('CLASS', axis=1)\n",
    "y_train = poker_after['CLASS']\n",
    "x_test = poker_test_after.drop('CLASS', axis=1)\n",
    "y_test = poker_test_after['CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At depth 0:\n",
      "Sample count: 25010\n",
      "Class distribution: [12493 10599  1206   513    93    54    36     6     5     5]\n",
      "Best split found:\n",
      "- Feature: 19\n",
      "- Threshold: 1\n",
      "- Information gain: 0.4304\n",
      "\n",
      "At depth 1:\n",
      "Sample count: 12650\n",
      "Class distribution: [12493     0     0     0    93    54     0     0     5     5]\n",
      "Best split found:\n",
      "- Feature: 17\n",
      "- Threshold: False\n",
      "- Information gain: 0.0146\n",
      "\n",
      "At depth 2:\n",
      "Sample count: 12547\n",
      "Class distribution: [12493     0     0     0     0    54]\n",
      "Best split found:\n",
      "- Feature: 18\n",
      "- Threshold: False\n",
      "- Information gain: 0.0086\n",
      "\n",
      "At depth 3:\n",
      "Sample count: 12493\n",
      "Class distribution: [12493]\n",
      "Stopping condition met! Creating leaf node.\n",
      "\n",
      "At depth 3:\n",
      "Sample count: 54\n",
      "Class distribution: [ 0  0  0  0  0 54]\n",
      "Stopping condition met! Creating leaf node.\n",
      "\n",
      "At depth 2:\n",
      "Sample count: 103\n",
      "Class distribution: [ 0  0  0  0 93  0  0  0  5  5]\n",
      "Best split found:\n",
      "- Feature: 18\n",
      "- Threshold: False\n",
      "- Information gain: 0.1315\n",
      "\n",
      "At depth 3:\n",
      "Sample count: 93\n",
      "Class distribution: [ 0  0  0  0 93]\n",
      "Stopping condition met! Creating leaf node.\n",
      "\n",
      "At depth 3:\n",
      "Sample count: 10\n",
      "Class distribution: [0 0 0 0 0 0 0 0 5 5]\n",
      "Best split found:\n",
      "- Feature: 4\n",
      "- Threshold: 0\n",
      "- Information gain: 0.3333\n",
      "\n",
      "At depth 4:\n",
      "Sample count: 6\n",
      "Class distribution: [0 0 0 0 0 0 0 0 1 5]\n",
      "Best split found:\n",
      "- Feature: 0\n",
      "- Threshold: 0\n",
      "- Information gain: 0.2778\n",
      "\n",
      "At depth 5:\n",
      "Sample count: 1\n",
      "Class distribution: [0 0 0 0 0 0 0 0 1]\n",
      "Stopping condition met! Creating leaf node.\n",
      "\n",
      "At depth 5:\n",
      "Sample count: 5\n",
      "Class distribution: [0 0 0 0 0 0 0 0 0 5]\n",
      "Stopping condition met! Creating leaf node.\n",
      "\n",
      "At depth 4:\n",
      "Sample count: 4\n",
      "Class distribution: [0 0 0 0 0 0 0 0 4]\n",
      "Stopping condition met! Creating leaf node.\n",
      "\n",
      "At depth 1:\n",
      "Sample count: 12360\n",
      "Class distribution: [    0 10599  1206   513     0     0    36     6]\n",
      "Best split found:\n",
      "- Feature: 20\n",
      "- Threshold: 1\n",
      "- Information gain: 0.1676\n",
      "\n",
      "At depth 2:\n",
      "Sample count: 11118\n",
      "Class distribution: [    0 10599     0   513     0     0     0     6]\n",
      "Best split found:\n",
      "- Feature: 19\n",
      "- Threshold: 2\n",
      "- Information gain: 0.0880\n",
      "\n",
      "At depth 3:\n",
      "Sample count: 10599\n",
      "Class distribution: [    0 10599]\n",
      "Stopping condition met! Creating leaf node.\n",
      "\n",
      "At depth 3:\n",
      "Sample count: 519\n",
      "Class distribution: [  0   0   0 513   0   0   0   6]\n",
      "Best split found:\n",
      "- Feature: 19\n",
      "- Threshold: 3\n",
      "- Information gain: 0.0229\n",
      "\n",
      "At depth 4:\n",
      "Sample count: 513\n",
      "Class distribution: [  0   0   0 513]\n",
      "Stopping condition met! Creating leaf node.\n",
      "\n",
      "At depth 4:\n",
      "Sample count: 6\n",
      "Class distribution: [0 0 0 0 0 0 0 6]\n",
      "Stopping condition met! Creating leaf node.\n",
      "\n",
      "At depth 2:\n",
      "Sample count: 1242\n",
      "Class distribution: [   0    0 1206    0    0    0   36]\n",
      "Best split found:\n",
      "- Feature: 19\n",
      "- Threshold: 2\n",
      "- Information gain: 0.0563\n",
      "\n",
      "At depth 3:\n",
      "Sample count: 1206\n",
      "Class distribution: [   0    0 1206]\n",
      "Stopping condition met! Creating leaf node.\n",
      "\n",
      "At depth 3:\n",
      "Sample count: 36\n",
      "Class distribution: [ 0  0  0  0  0  0 36]\n",
      "Stopping condition met! Creating leaf node.\n"
     ]
    }
   ],
   "source": [
    "tree_gini = DecisionTree(max_depth=5)\n",
    "tree_gini.fit(x_train, y_train, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
